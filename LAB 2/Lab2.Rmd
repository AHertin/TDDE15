---
title: "LAB 2"
author: "Andreas Hertin"
date: '2021-09-16'
output: word_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
set.seed(10)
library(HMM)
library("entropy")

```

The purpose of the lab is to put in practice some of the concepts covered in the lectures. To do so, you are asked to model the behavior of a robot that walks around a ring. The ring is divided into 10 sectors. At any given time point, the robot is in one of the sectors and decides with equal probability to stay in that sector or move to the next sector. You do not have direct observation of the robot. However, the robot is equipped with a tracking device that you can access. The device is not very accurate though: If the robot is in the sector i, then the device will report that the robot is in the sectors [i âˆ’ 2, i + 2] with equal probability

## (1)

Build a hidden Markov model (HMM) for the scenario described above.

```{r}

states = 1:10
symbols = 1:10

# Create transition matrix to match description
transMatrix <- diag(0.5, 10)
transMatrix[10, 1] = 0.5
diag(transMatrix[, -1]) = 0.5

emissMatrix <- matrix(0,10,10)

startMatrix <- rep(0.1,10)


for(i in 1:10){
  # get Y values that should be filled with 0.2
  lowerBound = i - 2
  upperBound = i + 2
  y_fill = lowerBound:upperBound%%10
  # Change 0 indexes to 10
  y_fill[y_fill == 0] = 10
  # Fill matrix by (i,y_fill) indexes with 0.2
  emissMatrix[i, y_fill] = 0.2
}

transMatrix
emissMatrix
startMatrix

hmmModel <- initHMM(States = states, Symbols = symbols, startProb = startMatrix, transProbs = transMatrix, emissionProbs = emissMatrix)
hmmModel

```


## (2)

Simulate the HMM for 100 time steps.

```{r}

N = 100

hmmModel.sim <- simHMM(hmm = hmmModel, length = N)

hmmModel.sim$states
hmmModel.sim$observation

```


## (3)

Discard the hidden states from the sample obtained above. Use the remaining observations to compute the filtered and smoothed probability distributions for each of the 100 time points. Compute also the most probable path.

```{r}

alpha <- exp(forward(hmmModel, hmmModel.sim$observation))
beta <- exp(backward(hmmModel, hmmModel.sim$observation))

filterFunc <- function(HMM, a, N){
  out = matrix(0, 10, N)
  for(i in 1:N){
    out[, i] = a[, i] / sum(a[, i])
  }
  return(out)
}

smoothFunc <- function(HMM, a, b, N){
  out = matrix(0, 10, N)
  for(i in 1:N){
    out[, i] = b[, i] * a[, i] / sum(b[, i] * a[, i])
  }
  return(out)
}

filtered <- filterFunc(hmmModel, alpha, N)
smoothed <- smoothFunc(hmmModel.sim, alpha, beta, N)
mostProbablePath <- viterbi(hmmModel, hmmModel.sim$observation)
mostProbablePath

```


## (4)

Compute the accuracy of the filtered and smoothed probability distributions, and of the most probable path. That is, compute the percentage of the true hidden states that are guessed by each method.

**Hint**: Note that the function ``forward()`` in the ``HMM`` package returns probabilities in log scale. You may need to use the functions ``exp()`` and ``prop.table()`` in order to obtain a normalized probability distribution. You may also want to use the functions ``apply()`` and ``which.max()`` to find out the most probable states. Finally, recall that you can compare two vectors A and B elementwise as A==B, and that the function ``table()`` will count the number of times that the different elements in a vector occur in the vector.


```{r}

filtered_pred <- apply(filtered, MARGIN = 2, which.max)
smoothed_pred <- apply(smoothed, MARGIN = 2, which.max)

filtered_pred
smoothed_pred
hmmModel.sim$states

accuracyFunc <- function(pred, true){
  confMatrix <- table(pred, true)
  accuracy <- sum(diag(confMatrix)) / sum (confMatrix)
  return(accuracy)
}

accFilt   <- accuracyFunc(filtered_pred, hmmModel.sim$states)
accSmooth <- accuracyFunc(smoothed_pred, hmmModel.sim$states)
accBase <- accuracyFunc(mostProbablePath, hmmModel.sim$states)

accFilt
accSmooth
accBase


```

The basic most-predictable-path has the lowest accuracy, followed by filterd and the the highest accuracy is obtained by the smoothing function. 


## (5)

Repeat the previous exercise with different simulated samples. In general, the smoothed distributions should be more accurate than the filtered distributions. Why ? In general, the smoothed distributions should be more accurate than the most probable paths, too. Why ?

```{r}

set.seed(9999)

# Makes task (1-4) as function to simulate for different amount of observations
simulateAccuracy <- function(HMM, N){
  HMM.sim <- simHMM(hmm = HMM, length = N)

  alpha <- exp(forward(HMM, HMM.sim$observation))
  beta <- exp(backward(HMM, HMM.sim$observation))
  
  filtered <- filterFunc(HMM, alpha, N)
  smoothed <- smoothFunc(HMM.sim, alpha, beta, N)
  
  mostProbablePath <- viterbi(HMM, HMM.sim$observation)
  
  filtered_pred <- apply(filtered, MARGIN = 2, which.max)
  smoothed_pred <- apply(smoothed, MARGIN = 2, which.max)
  
  accFilt <- accuracyFunc(filtered_pred, HMM.sim$states)
  accSmooth <- accuracyFunc(smoothed_pred, HMM.sim$states)
  accBase <- accuracyFunc(mostProbablePath, HMM.sim$states)
  
  out <- matrix(c(accBase, accSmooth, accFilt),1,3)
  return(out)
}

nSim = 100

HMM <- initHMM(States = states, Symbols = symbols, startProb = startMatrix, transProbs = transMatrix, emissionProbs = emissMatrix)

accuracyMatrix_25 <- matrix(0, nSim, 3)
accuracyMatrix_50 <- matrix(0, nSim, 3)
accuracyMatrix_200 <- matrix(0, nSim, 3)

# Simulate nSim times with different number of observevations.
for(n in 1:nSim){
  accuracyMatrix_25[n, ] <- simulateAccuracy(HMM, 25)
  accuracyMatrix_50[n, ] <- simulateAccuracy(HMM, 50)
  accuracyMatrix_200[n, ] <- simulateAccuracy(HMM, 200)
}

# Plots of accuracy distribution for different amount of observations
plot(density(accuracyMatrix_25[, 1]), main = paste("Acccuracy distributions for 25 observations (simulated 100 times)"), font.main = 1, cex.main = 0.8, xlim=c(0.2,1), xlab="Accuracy", ylim=c(0,10), sub="black - Base, red - Smoothed, green - Filtered")
lines(density(accuracyMatrix_25[, 2]), col="red")
lines(density(accuracyMatrix_25[, 3]), col="green")

plot(density(accuracyMatrix_50[, 1]), main = "Acccuracy distributions for 50 observations (simulated 100 times)", font.main = 1, cex.main = 0.8, xlim=c(0.2,1), xlab="Accuracy", ylim=c(0,10), sub="black - Base, red - Smoothed, green - Filtered")
lines(density(accuracyMatrix_50[, 2]), col="red")
lines(density(accuracyMatrix_50[, 3]), col="green")

plot(density(accuracyMatrix_200[, 1]), main = paste("Acccuracy distributions for 200 observations (simulated 100 times)"), font.main = 1, cex.main = 0.8, xlim=c(0.2,1), xlab="Accuracy", ylim=c(0,10), sub="black - Base, red - Smoothed, green - Filtered")
lines(density(accuracyMatrix_200[, 2]), col="red")
lines(density(accuracyMatrix_200[, 3]), col="green")


```

No matter number of observations the same order of accuracy is seen. This is logical because filter takes alpha (forward observations) and smoothing uses both alpha and beta (backward observations).


## (6)

Is it always true that the later in time (i.e., the more observations you have received) the better you know where the robot is?

**Hint**: You may want to compute the entropy of the filtered distributions with the function ``entropy.empirical()`` of the package ``entropy``.

```{r}

n = 250

eHMM <- initHMM(States = states, Symbols = symbols, startProb = startMatrix, transProbs = transMatrix, emissionProbs = emissMatrix)
eHMM.sim <- simHMM(hmm = eHMM, length = n)

alpha <- exp(forward(eHMM, eHMM.sim$observation))
beta <- exp(backward(eHMM, eHMM.sim$observation))

eFiltered <- filterFunc(eHMM, alpha, n)

eFilteredPred <- apply(eFiltered, MARGIN = 2, which.max)

entropyOut = rep(0,200)

for(f in 1:length(eFilteredPred)){
  entropyOut[f] <- entropy.empirical(table(factor(eFilteredPred[1:f])))
}

plot(entropyOut, type = "l", main = "Entropy from 0-250 observations", xlab = "N observations", ylab = "Entropy")

plot(density(accuracyMatrix_25[, 2]), main = paste("Smoothing accuracy distribution for different number of observations"), font.main = 1, cex.main = 0.8, xlim=c(0.2,1), xlab="Accuracy", ylim=c(0,10), sub="black - 25, red - 50, green - 200")
lines(density(accuracyMatrix_50[, 2]), col="red")
lines(density(accuracyMatrix_200[, 2]), col="green")

```

Entropy converges just before 50 observations and higher entropy = more disorder. Accuracy does not increase with number of observations as seen in the last plot, but it does increase precision.


## (7)

Consider any of the samples above of length 100. Compute the probabilities of the hidden states for the time step 101.

```{r}

step_101 <- t(smoothed[, 100]%*%transMatrix)
step_101

```


